# Tareas predictivas

Los modelos de predicci√≥n cl√≠nica son herramientas fundamentales para estimar resultados de salud en pacientes, ya sea en el presente (**diagn√≥stico**) o en el futuro (**pron√≥stico**). Tradicionalmente, estos modelos se basaban en an√°lisis de regresi√≥n multivariable, pero el avance del aprendizaje autom√°tico ha permitido el desarrollo de enfoques m√°s sofisticados con una mayor capacidad de procesamiento y precisi√≥n. Su aplicaci√≥n en la medicina personalizada ha crecido exponencialmente, facilitando la toma de decisiones mediante la estimaci√≥n del riesgo individualizado.

Sin embargo, para que un modelo predictivo sea √∫til en la pr√°ctica cl√≠nica, debe someterse a una rigurosa evaluaci√≥n de su validez e impacto. A pesar de la existencia de m√∫ltiples gu√≠as metodol√≥gicas, muchas herramientas predictivas no avanzan m√°s all√° de su fase de desarrollo inicial o presentan limitaciones en su aplicaci√≥n real. Adem√°s, la continua creaci√≥n de nuevos modelos, en lugar de la optimizaci√≥n de los ya existentes, genera redundancia y desperdicio de recursos. Por lo tanto, es crucial avanzar en la validaci√≥n, actualizaci√≥n y adaptaci√≥n de modelos predictivos, garantizando su aplicabilidad en diversos entornos cl√≠nicos y poblacionales.

::: {style="text-align: center;"}
![Marco desde la derivaci√≥n hasta la implementaci√≥n de modelos de predicci√≥n cl√≠nica.](fig/Articulo_cap1_8.png){width="400" height="250"}
:::

**Ejemplo de Predicci√≥n para Diagn√≥stico**

Contexto: Un m√©dico quiere identificar si un paciente tiene diabetes bas√°ndose en sus caracter√≠sticas cl√≠nicas.

üîπ Pregunta: ¬øEste paciente tiene diabetes en este momento?\
üîπ Datos: Edad, nivel de glucosa en sangre, presi√≥n arterial, √≠ndice de masa corporal (IMC), antecedentes familiares de diabetes.\
üîπ Modelo: Algoritmo de clasificaci√≥n (Regresi√≥n log√≠stica, √Årboles de decisi√≥n, Redes neuronales).\
üîπ Salida esperada: Probabilidad de que el paciente tenga diabetes (S√≠/No).

üí° Ejemplo real: El modelo FINDRISC es una herramienta usada para predecir el riesgo de diabetes tipo 2 a partir de variables cl√≠nicas y de estilo de vida.

**Ejemplo de Predicci√≥n para Pron√≥stico**

Contexto: Un onc√≥logo desea estimar el riesgo de recurrencia del c√°ncer en un paciente tratado con cirug√≠a.

üîπ Pregunta: ¬øCu√°l es la probabilidad de que el c√°ncer reaparezca en los pr√≥ximos 5 a√±os?\
üîπ Datos: Edad, tipo de tumor, nivel de marcadores tumorales, estado de ganglios linf√°ticos, tratamiento recibido.\
üîπ Modelo: Modelos de supervivencia (Regresi√≥n de Cox, Redes neuronales profundas).\
üîπ Salida esperada: Probabilidad de recurrencia en distintos per√≠odos de tiempo (6 meses, 1 a√±o, 5 a√±os).

üí° Ejemplo real: PREDICT es un modelo que estima la supervivencia en pacientes con c√°ncer de mama basado en caracter√≠sticas cl√≠nico-patol√≥gicas.

::: callout-warning
## Diferencia clave:

-   üìå *Diagn√≥stico* ‚Üí Se enfoca en el presente: determinar si una enfermedad est√° presente o no.

-   üìå *Pron√≥stico* ‚Üí Se enfoca en el futuro: predecir la evoluci√≥n o el desenlace de una enfermedad.
:::

## Validaci√≥n

La validaci√≥n es el proceso de evaluar el rendimiento de un modelo en entornos espec√≠ficos. Existen dos tipos principales de validaci√≥n: interna y externa.

::: {style="text-align: center;"}
![Marco desde la derivaci√≥n hasta la implementaci√≥n de modelos de predicci√≥n cl√≠nica.](fig/Articulo_cap1_11.png)
:::


#### Validaci√≥n Interna

Eval√∫a la reproducibilidad del modelo en la misma poblaci√≥n de los datos de derivaci√≥n. Se utilizan t√©cnicas de remuestreo como bootstrapping o validaci√≥n cruzada para evaluar el sobreajuste y aplicar correcciones si es necesario. No se recomienda la simple divisi√≥n de datos en conjuntos de entrenamiento y prueba, ya que limita la muestra y puede generar estimaciones imprecisas.

#### Validaci√≥n Externa

Mide la capacidad del modelo para generalizarse a nuevas poblaciones o entornos. Se distinguen tres tipos principales:

-   **Validaci√≥n temporal**: Evaluaci√≥n en datos de un per√≠odo m√°s reciente.

-   **Validaci√≥n geogr√°fica**: Prueba en distintas ubicaciones.

-   **Validaci√≥n de dominio**: Aplicaci√≥n en diferentes entornos cl√≠nicos.

La validaci√≥n de dominio es la m√°s rigurosa, pues implica evaluar el desempe√±o del modelo en entornos muy distintos.

### Cu√°ndo y C√≥mo Validar

Un modelo debe validarse internamente antes de ser probado en entornos externos. No hay consenso sobre cu√°ntas validaciones acumulativas son necesarias. Se recomienda un tama√±o de muestra suficiente (por ejemplo, al menos 100 eventos y 100 no eventos) para asegurar precisi√≥n en las estimaciones de desempe√±o. M√©todos m√°s avanzados pueden mejorar la evaluaci√≥n de la validez mediante estimaciones de medidas de rendimiento en lugar de c√°lculos de potencia.

#### Evaluaci√≥n de la Validez

Idealmente, la validaci√≥n externa debe ser realizada por investigadores independientes. Comparar el contexto de los datos de derivaci√≥n y validaci√≥n ayuda a evaluar la transportabilidad del modelo. La heterogeneidad en la poblaci√≥n de validaci√≥n fortalece la prueba del modelo.

Se pueden analizar distintos aspectos del desempe√±o:

1.  **Discriminaci√≥n**: Capacidad del modelo para separar sujetos con diferentes resultados. Se eval√∫a con la estad√≠stica de concordancia (C) o AUROC. Otras medidas incluyen la estad√≠stica D y la pendiente de discriminaci√≥n.

2.  **Calibraci√≥n**: Grado de concordancia entre riesgos predichos y observados. Se analizan gr√°ficos de calibraci√≥n y medidas como la pendiente de calibraci√≥n y la relaci√≥n O/E.

3.  **Rendimiento general**: Incluye medidas como la variaci√≥n explicada (R¬≤) y la puntuaci√≥n Brier.

### Determinar si un Modelo es "Suficientemente Bueno"

Un modelo debe evaluarse considerando discriminaci√≥n y calibraci√≥n. Si se prioriza la identificaci√≥n de sujetos de alto riesgo, la discriminaci√≥n es clave. Si el modelo presenta mala calibraci√≥n, se puede mejorar mediante recalibraci√≥n. La adecuaci√≥n del desempe√±o depende del contexto de aplicaci√≥n y debe evaluarse con medidas de decisi√≥n anal√≠tica, como el Beneficio Neto.

## Evaluaci√≥n de Impacto

### ¬øQu√© es la Evaluaci√≥n de Impacto?

Los modelos con un buen desempe√±o estad√≠stico no garantizan autom√°ticamente un impacto positivo en la pr√°ctica cl√≠nica. Aunque un modelo tenga una buena discriminaci√≥n y calibraci√≥n, puede ser cl√≠nicamente in√∫til si no aporta informaci√≥n novedosa o si sus predicciones no son relevantes en el contexto de la toma de decisiones cl√≠nicas. Factores externos, como interpretaciones cl√≠nicas, adherencia o aceptabilidad, pueden afectar el impacto real de un modelo. La evaluaci√≥n de impacto analiza las consecuencias de utilizar un modelo en la pr√°ctica cl√≠nica, considerando cambios en la toma de decisiones, resultados en los pacientes y costos de atenci√≥n m√©dica.

### ¬øC√≥mo se Puede Evaluar el Impacto?

La evaluaci√≥n de impacto puede ser potencial o real.

::: {style="text-align: center;"}
![Marco desde la derivaci√≥n hasta la implementaci√≥n de modelos de predicci√≥n cl√≠nica.](fig/Articulo_cap1_9.png)
:::


-   **Impacto Potencial:** Examina el impacto te√≥rico a trav√©s de medidas de desempe√±o cl√≠nico y modelos de an√°lisis de decisiones.

    -   **Utilidad Cl√≠nica:** Se eval√∫a con medidas como sensibilidad, especificidad y el Beneficio Neto, que pondera los beneficios y los da√±os cl√≠nicos.

    -   **An√°lisis Econ√≥mico de la Salud:** Utiliza modelos de an√°lisis de decisiones (Markov, √°rboles de decisi√≥n) para evaluar la relaci√≥n costo-efectividad.
    
::: callout-note
## ¬°Atenci√≥n!

1. Los estudios que evaluan la exactitud diagn√≥stica de cualquier test, marcador, sintoma, signo, etc, pertenece a estudios predictivos que intentan evaluar el impacto potencial.

2. Debido a que la sensibilidad y especificidad solo valoran el impacto potencial, estos son considerados como desenlaces sub-rogados en las preguntas cl√≠nicas de las Gu√≠as de Pr√°ctica Cl√≠nica basadas en evidencia.
:::


-   **Impacto Real:** Se mide en estudios emp√≠ricos, como ensayos aleatorizados, comparando grupos con y sin el uso del modelo. Puede evaluarse de manera asistida (dejando espacio para la interpretaci√≥n cl√≠nica) o directiva (sugiriendo decisiones espec√≠ficas basadas en el modelo).

::: callout-note
## ¬°Atenci√≥n!

Son muy escasos los estudios que evaluan el impacto real mediante un ECA y se han propuesto otras alternativas. [Consulta m√°s informaci√≥n aqu√≠](https://doi.org/10.1093/ndt/gfae170)
:::


### ¬øCu√°ndo se Debe Evaluar el Impacto?

No todos los modelos requieren evaluaci√≥n de impacto. Es esencial para modelos que gu√≠an decisiones terap√©uticas o de diagn√≥stico. Se recomienda realizarla despu√©s de la validaci√≥n externa y antes de la implementaci√≥n.

## Actualizaci√≥n de Modelos

### ¬øQu√© es la Actualizaci√≥n de Modelos y Cu√°ndo es √ötil?

Frecuentemente se derivan nuevos modelos en lugar de aprovechar los existentes. Reutilizar y actualizar modelos previos es m√°s eficiente y evita sobreajustes. La actualizaci√≥n es recomendable cuando un modelo existente tiene buena discriminaci√≥n pero problemas de calibraci√≥n, cuando surgen nuevos predictores relevantes o cuando hay cambios en la poblaci√≥n objetivo.

::: {style="text-align: center;"}
![Marco desde la derivaci√≥n hasta la implementaci√≥n de modelos de predicci√≥n cl√≠nica.](fig/Articulo_cap1_10.png)
:::


### ¬øC√≥mo se Pueden Actualizar los Modelos?

Existen cuatro enfoques principales para la actualizaci√≥n de modelos basados en regresi√≥n:

1.  **Recalibraci√≥n:** Ajusta la intersecci√≥n del modelo o reestima el predictor lineal para corregir descalibraciones menores.

2.  **Revisi√≥n del Modelo:** Reestima algunos o todos los coeficientes del modelo original para reflejar mejor la nueva poblaci√≥n.

3.  **Extensi√≥n del Modelo:** Agrega nuevos predictores relevantes, evitando sobreajustes mediante t√©cnicas de contracci√≥n.

4.  **Actualizaci√≥n de Metamodelos:** Combina varios modelos existentes en uno m√°s generalizable, utilizando enfoques metaanal√≠ticos o de regresi√≥n apilada.

Cada enfoque se elige seg√∫n la magnitud del cambio requerido y la disponibilidad de datos. Idealmente, los modelos actualizados deben someterse a una evaluaci√≥n adicional para validar su desempe√±o y utilidad cl√≠nica.

## Referencias

1.  Conroy, S., Murray, E.J. Let the question determine the methods: descriptive epidemiology done right. Br J Cancer 123, 1351‚Äì1352 (2020). <https://doi.org/10.1038/s41416-020-1019-z>

2.  Hern√°n, M. A., Hsu, J., & Healy, B. A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks. CHANCE, 2019; 32(1), 42‚Äì49. <https://doi.org/10.1080/09332480.2019.1579578>

3. Binuya MAE, Engelhardt EG, Schats W, Schmidt MK, Steyerberg EW. Methodological guidance for the evaluation and updating of clinical prediction models: a systematic review. BMC Med Res Methodol. 2022;22(1):316. Published 2022 Dec 12. <https://doi.org/10.1186/s12874-022-01801-8>

4. Roemer J Janse, Vianda S Stel, Kitty J Jager, Giovanni Tripepi, Carmine Zoccali, Friedo W Dekker, Merel van Diepen, When impact trials are not feasible: alternatives to study the impact of prediction models on clinical practice, Nephrology Dialysis Transplantation, Volume 40, Issue 1, January 2025, Pages 27‚Äì33, <https://doi.org/10.1093/ndt/gfae170>

## Disclaimer

-   Esta secci√≥n fue editada usando ChatGPT.
